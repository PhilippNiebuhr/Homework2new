---
title: "Session 4: Homework 2"
author: "Study Group 15A"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
```



# Climate change and temperature anomalies 

**Loading File**

```{r weather_data, cache=TRUE}
weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

inspect(weather)
```
# Convert the dataframe from wide to 'long' format.

```{r tidyweather}
tidyweather <- weather %>%
  select(Year:Dec) %>%
  pivot_longer(cols=2:13, names_to = "month" ,values_to = "delta")
  #gather(2:13, key = "Month", value = "delta") #another way to do it, orders by month

inspect(tidyweather)
```

## Plotting Data on time-series scatter plot

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")), #ymd takes different years, months and days & formats it into year-month-day
         month = month(date, label=TRUE),
         year = year(date)) #I believe using month() and date() here changes the type of the variables

inspect(tidyweather)

tidyweather_plot <- ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_economist_white() +
  labs (
    title = "Sharp Increase in Weather Anomalies Over Time",
    subtitle = "Deviations From the Base Period 1951-1980",
    x = "",
    y = "Temperature Deviations",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.txt"
  )

tidyweather_plot

```


```{r facet_wrap}
tidyweather_plot_month <- 
  tidyweather_plot +
  facet_wrap(~month) +
  theme_economist_white() +
  labs(
    title = "Sharp Increase in Weather Anomalies Over Time",
    subtitle = "Deviations From the Base Period 1951-1980, for Each Month",
    caption = "Source: https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.txt"
       )
 
tidyweather_plot_month
```
**From the graph above, it seems like the months surrounding and including winter i.e. oct, nov, dec, jan, feb, mar, have seen a slightly greater change than the months closer to spring and summer (start slightly lower and/or end slightly higher). The data for these months also seems more spread out (grey error ribbon around smoothing line is wider)**


**It is sometimes useful to group data into different time periods to study historical data. For example, we often refer to decades such as 1970s, 1980s, 1990s etc. to refer to a period of time. NASA calculates a temperature anomaly, as difference form the base period of 1951-1980. The code below creates a new data frame called `comparison` that groups data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present. **

**We remove data before 1800 and before using `filter`. Then, we use the `mutate` function to create a new variable `interval` which contains information on which period each observation belongs to. We can assign the different periods using `case_when()`.**


```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  )) #case_when vectorises if else statements i.e. if Year is between 1881:1920, then call new data 1881-1920, else if year is between 1921:1950, then call etc...
#TRUE takes all the cases which weren't considered in the previous if statements and assigns them to 2011-present

```

**Now that we have the `interval` variable, we can create a density plot to study the distribution of monthly deviations (`delta`), grouped by the different time periods we are interested in. Set `fill` to `interval` to group and colour the data by different time periods.**

```{r density_plot}
ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.35) +   #density plot with transparency set to 20%
  theme_economist_white() +                #theme
  labs (
    title = "The Planet's Getting Hotter!",
    subtitle = "Density Plot for Monthly Temperature Anomalies",
    x = "Temperature Deviations",
    y     = "Density"         #changing y-axis label to sentence case
  )

```

**So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result.**

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y=annual_average_delta))+ #changed year to Year
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_smooth(method = "loess") +
  
  #change to theme_bw() to have white background + black frame around plot
  theme_economist_white() +
  labs (
    title = "Temperature Anomalies are on the Rise...",
    subtitle = "...And Faster than Ever",
    x = "",
    y     = "Average Annual Temperature Deviations"
  )                         


```


## Confidence Interval for `delta`


**A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.**



**In addition to finding the total CIs for the 2011-2019 comparison dataset, we decided it would be interesting to compare the different confidence intervals across the specific years within 2011-2019, and so found the CIs and the means across this year range and plotted them.**

```{r, calculate_CI_using_formula}

formula_ci <- comparison %>%
              filter(interval=="2011-present") %>% 
              subset(select=-c(4:6)) %>% #getting rid of unnecessary columns
              group_by(Year) %>% #averaging by year
              summarise(average_annual_anomaly=mean(delta,na.rm=TRUE), #averaging by year
                        sd=sd(delta,na.rm=TRUE),
                        count=count(Year),
                        t_critical=qt(0.975,count-1), 
                        se=sd/sqrt(count),
                        CI_low=average_annual_anomaly-t_critical*se,
                        CI_high=average_annual_anomaly+t_critical*se)
              
formula_ci

ggplot(formula_ci, aes(x=reorder(Year,Year), y=average_annual_anomaly)) +
  geom_point() +
  geom_errorbar(width=.2, aes(ymin=CI_low, ymax=CI_high)) + 
  labs(x=" ", y= "Average annual deviation", title="Highest deviation from base period in 2016", 
       subtitle="Confidence intervals of yearly temperature deviations") + 
  theme_economist()+
  coord_flip()

formula_ci_total <- comparison %>% 
                    filter(interval=="2011-present") %>% 
                    subset(select=-c(4:6)) %>%
                    summarise(average_annual_anomaly=mean(delta,na.rm=TRUE), #averaging by year
                        sd=sd(delta,na.rm=TRUE),
                        count=n(),
                        t_critical=qt(0.975,count-1), 
                        se=sd/sqrt(count),
                        CI_low=average_annual_anomaly-t_critical*se,
                        CI_high=average_annual_anomaly+t_critical*se)
                                 
formula_ci_total

```


```{r, calculate_CI_using_bootstrap}

library(infer)
formula_ci2 <- comparison %>% 
  filter(interval == "2011-present") %>%   # choose the interval 2011-present
  specify(response = delta) %>%    
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean") %>%
  get_confidence_interval(level=0.95, type = "percentile")

formula_ci2

```

> What is the data showing us? Please type your answer after (and outside!) this blockquote. You have to explain what you have done, and the interpretation of the result. One paragraph max, please!

**First, we use filter to pick the targeted interval from 2011 onwards. Then, we use both formula method and bootstrap method to generate the 95% confidence interval of delta, which lies between 0.916 to 1.02. This indicates we are 95% confident that the interval (0.916 - 1.02) captures the true mean value of delta. We can use that to infer the potential trend of the temperature deviation.**


# General Social Survey (GSS)


```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable"))

inspect(gss)
skim(gss)
```

## Instagram and Snapchat, by sex

#Can we estimate the *population* proportion of Snapchat or Instagram users in 2016?

#1. Create a  new variable, `snap_insta` that is *Yes* if the respondent reported using any of Snapchat (`snapchat`) or Instagram (`instagrm`), and *No* if not. If the recorded value was NA for both of these questions, the value in your new variable should also be NA.

```{r, snap_insta, cache=TRUE}
gss_new <- gss %>%
  mutate(snap_insta = case_when(snapchat == "Yes" | instagrm == "Yes" ~ "Yes",
                                snapchat == "No" & instagrm == "No" ~ "No",
                                TRUE ~ "NA"))
gss_new
```

#1.Calculate the proportion of Yes’s for `snap_insta` among those who answered the question, i.e. excluding NAs.

```{r, snap_insta_yes, cache=TRUE}
gss_snap_insta <- gss_new %>%
  filter(snap_insta != "NA") %>%
  group_by(snap_insta) %>%
  summarise(count_snap_insta = count(snap_insta)) %>%
  mutate(proportion = count_snap_insta/sum(count_snap_insta))

gss_snap_insta
```

#1. Using the CI formula for proportions, please construct 95% CIs for men and women who used either Snapchat or Instagram

```{r, snap_insta_CI, cache=TRUE}
gss_snap_insta_ci <- gss_snap_insta %>%
  mutate(Lower_Confidence_Interval = proportion + qnorm(0.025)*sqrt(proportion*(1-proportion)/1372), Higher_Confidence_Interval = proportion + qnorm(0.975)*sqrt(proportion*(1-proportion)/1372), Low_Count = Lower_Confidence_Interval*1372, High_Count = Higher_Confidence_Interval*1372, snap_insta = c("Use Neither Snapchat nor Instagram", "Use One or Both of Snapchat or Instagram"))

gss_snap_insta_ci

gss_snap_insta_ci_plot <- gss_snap_insta_ci %>%
  ggplot(aes(x = snap_insta, y = count_snap_insta, fill = snap_insta)) +
  geom_col(show.legend = FALSE) +
  geom_errorbar(width=.2, aes(ymin=Low_Count, ymax=High_Count)) +
  geom_text(aes(label = signif(proportion, digits = 3), vjust = -0.5,  hjust = 6)) + 
  labs(title = "Men and Women say NO to Instagram and Snapchat*", 
       subtitle = "*Well at least a large proportion of the ones who took this survey do", 
       x = "",
       y = "Number of respondants") 
  

gss_snap_insta_ci_plot
```

## Twitter, by education level

#Can we estimate the *population* proportion of Twitter users by education level in 2016?. 

**There are 5 education levels in variable `degree` which, in ascending order of years of education, are Lt high school, High School, Junior college, Bachelor, Graduate.**

#1. Turn `degree` from a character variable into a factor variable. Make sure the order is the correct one and that levels are not sorted alphabetically which is what R by default does. 

```{r, degree_chr_to_fct, cache=TRUE}
gss_deg_fac <- ordered(gss$degree, levels = c("Lt high school", "High school", "Junior College", "Bachelor", "Graduate"))

gss_new <- gss[-7] %>% 
  mutate(gss_deg_fac)

skim(gss_new)
```

#1. Create a new variable, `bachelor_graduate` that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree. As before, if the recorded value for either was NA, the value in your new variable should also be NA.

```{r, degree_bachelor_graduate, cache=TRUE}
gss_bg <- gss_new %>% 
 mutate(bachelor_graduate = case_when(
    gss_deg_fac %in% c("Bachelor", "Graduate") ~ "Yes",
    gss_deg_fac %in% NA ~ "NA",
    TRUE ~ "No"
  ))

gss_bg

```

#1. Calculate the proportion of `bachelor_graduate` who do (Yes) and who don't (No) use twitter. 

```{r, degree_bg_twitter, cache=TRUE}
bg_prop <- gss_bg %>% 
  filter(bachelor_graduate == "Yes") %>% 
  filter(twitter != "NA") %>% 
  group_by(twitter) %>% 
  summarise(count_twitter = count(twitter)) %>% 
  mutate(proportion = count_twitter/sum(count_twitter)) 

bg_prop

bg_prop_graph <- bg_prop %>% 
  ggplot(aes(x=twitter, y = count_twitter, fill = twitter)) +
  labs(
    title = "People Who Hold a Bachelor Level Degree or Higher Use Twitter Less",
    subtitle = "According to Data from this Survey",
    x = "Use Twitter?",
    y = "Number of People From Survey"
  ) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = signif(proportion, digits = 3), vjust = -0.5)) + #tried to format this as percentage, but can't figure out how?
  theme_economist_white() +
  theme(
  plot.title = element_text(vjust = 2, size = 25),
  plot.subtitle = element_text(vjust = 1, size = 20),
  axis.title.y = element_text(size=14,face="bold", vjust = 2),
  axis.title.x = element_text(size=14,face="bold")
)

bg_prop_graph

```

#1. Using the CI formula for proportions, please construct two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter. 

```{r, degree_bg_CI, cache=TRUE}
sd_prop <- function(x,y) sqrt(x*(1-x)/y)
CI <- function(x,y,z) x + qnorm(z)*sd_prop(x,y)
sum_sample <- sum(bg_prop$count_twitter)


bg_prop_CI <- bg_prop %>% 
  mutate(SD = sd_prop(proportion, sum_sample), CI_low = CI(proportion, sum_sample, 0.025), CI_high = CI(proportion, sum_sample, 0.975), CI_low_abs = CI_low*sum_sample, CI_high_abs = CI_high*sum_sample) 

bg_prop_CI

bg_prop_CI_plot <- bg_prop_CI %>% 
  ggplot(aes(x=twitter, y = count_twitter, fill = twitter)) +
  labs(
    title = "People Who Hold a Bachelor Level Degree or Higher Use Twitter Less",
    subtitle = "According to Data from this Survey",
    x = "Use Twitter?",
    y = "Number of People From Survey"
  ) +
  geom_col(show.legend = FALSE) +
  geom_errorbar(width=.2, aes(ymin=CI_low_abs, ymax=CI_high_abs)) +
  geom_text(aes(label = signif(proportion, digits = 3), vjust = -0.5, hjust = 6)) + #tried to format this as percentage, but can't figure out how?
  theme_economist_white() +
  theme(
  plot.title = element_text(vjust = 2, size = 25),
  plot.subtitle = element_text(vjust = 1, size = 20),
  axis.title.y = element_text(size=14,face="bold", vjust = 2),
  axis.title.x = element_text(size=14,face="bold")
)

bg_prop_CI_plot
```


#1. Do these two Confidence Intervals overlap? 

**No these confidence intervals do not overlap. They do have the same size though, due to the symmetry of the equation for the standard deviation for proportions (i.e. x(1-x) will be the same for both Yes and No, since p(yes) + p(no) = 1).**


## Email usage

#Can we estimate the *population* parameter on time spent on email weekly?

### 1. Create a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly.

**In the following, we transformed the given data set *gss* into the new table *email_comb*, in which we have cleaned the data (ommiting all NAs), converted both columns of interst email_hr and email_min into doubles, and then combined the columns. Additioanly, we present the summary statistics using both a manual approach and the *inspect()* command.**

```{r, email_usage}

#Original data set
gss 

#combine emailhr and emailmin and convert them to doubles
email_comb <- gss %>% mutate(emailmin_new=as.double(emailmin)/60,
                                      emailhr_new=as.double(emailhr),
                                      email_time=emailhr_new+emailmin_new) %>% 
                      na.omit() %>% 
                      select(email_time)

email_comb

#basic statistics about hours spent checking emails 
mean_median_time <- email_comb %>% 
                    summarise(mean_hours=mean(email_time),
                              median_hours=median(email_time))
mean_median_time #manual approach

inspect(email_comb) #inspect command

# Percentages
8/1649
3/1649
  
  
```

### 2. Visualise the distribution of this new variable. Find the mean and the median number of minutes respondents spend on email weekly. Is the mean or the median a better measure of the typical amounT of time Americans spend on email weekly? Why?

**Both the histogram and the boxplot reveal a heavily right-skewed distribution. Most of the survey participants spend well below 10 hours per week checking their emails. However, some of those surveyed indicated to spend much more time per week on this task: 8 people (representing 0.5% of all participants) spend over 50 hours per week, 3 (0.2%) spent even more than 75 hours per week. Those outliers pull the sample mean upwards, making the median a better, more robust, and more representative measure of the "typical value" in the sample.**

```{r, email_usage_2}

# Visualization of distribution: Histogram
ggplot(email_comb, aes(x=email_time))+
 geom_histogram() +
theme_economist_white() + 
  labs (title="Most People Spend Little Time Checking their Mails",
        subtitle = "Hours People Spent Checking their Emails per Week", 
        x="Hours spent per week", 
        y = "Number of Persons")

# Visualization of distribution: Boxplot

ggplot(email_comb, aes(email_time)) +
       labs(title = "Heavy outliers pull the mean upwards",
            subtitle = "Boxplot",
            x = "Hours spent per week",
            y = "") + 
         theme_economist_white() + theme(axis.text.y=element_blank()) +
        geom_boxplot(width=1) + ylim(-1,1) +
        NULL

```

### 3. Using the `infer` package, calculate a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. Interpret this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes). If you get a result that seems a bit odd, discuss why you think this might be the case.


```{r, email_usage3}

infer_ci_email <- email_comb %>% 
  specify(response=email_time) %>% 
  generate(reps=1000,type="bootstrap") %>% 
  calculate(stat="mean") %>% get_confidence_interval(level=0.95, type="percentile")
infer_ci_email 
  
```


### 4. Would you expect a 99% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.

**A 99% confidence interval will be wider than a 95% confidence interval because to be more confident that the true population value falls within the interval we will need to allow more potential values within the interval. That’s because a confidence level refers to the percentage of all possible samples that can be expected to include the true population parameter. For example, suppose all possible samples were selected from the same population, and a confidence interval computed for each sample. A 95% confidence level implies that 95% of the confidence intervals would include the true population parameter. 5% of the intervals won’t contain the parameter. The confidence level most commonly adopted is 95%. Comparing the output above (95% CI) with the output below (99% CI) shows that the interval increases when increasing the confidence level.**

```{r, email_usage4}

infer_ci_email_99 <- email_comb %>% 
  specify(response=email_time) %>% 
  generate(reps=1000,type="bootstrap") %>% 
  calculate(stat="mean") %>% get_confidence_interval(level=0.99, type="percentile")
infer_ci_email_99
  
```


# Trump's Approval Margins

##As we saw in class, fivethirtyeight.com has detailed data on [all polls that track the president's approval ](https://projects.fivethirtyeight.com/trump-approval-ratings)

```{r, cache=TRUE}
# Import approval polls data
approval_polllist <- read_csv(here::here('data', 'approval_polllist.csv'))

# or directly off fivethirtyeight website
# approval_polllist <- read_csv('https://projects.fivethirtyeight.com/trump-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)

approval_polllist<-approval_polllist %>% 
  mutate(end_date = mdy(enddate))

# Use `lubridate` to fix dates, as they are given as characters.
```

## Create a plot

What I would like you to do is to calculate the average net approval rate (approve- disapprove) for each week since he got into office. I want you plot the net approval, along with its 95% confidence interval. There are various dates given for each poll, please use `enddate`, i.e., the date the poll ended.

You can facet by year, and add an orange line at zero. Your plot should look like this:

```{r trump_margins, echo=FALSE, out.width="100%"}

knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
library(sjstats)

approval_polllist <- read_csv(here::here('data', 'approval_polllist.csv'))
approval_polllist

net_approval_rate_df <- approval_polllist %>%
  mutate(enddate = mdy(enddate),net_approval_rate=approve-disapprove, week=week(enddate),year=as.character(year(enddate))) %>%
  select(-c(1:4, 6:22))
net_approval_rate_df

#Calculating CIs
net_approval_rate_mean_CI <- net_approval_rate_df %>%
  group_by(year,week) %>%
  summarise(average_weekly_net_approval=mean(net_approval_rate),
            sd=sd(net_approval_rate),
            count=count(year),
            t_critical=qt(0.975,count-1),
            se=sd/sqrt(count),
            CI_low=average_weekly_net_approval-t_critical*se,
            CI_high=average_weekly_net_approval+t_critical*se) %>%
  view()


ggplot(net_approval_rate_mean_CI, aes(x=week, y=average_weekly_net_approval,color=year))+
  geom_line(show.legend=FALSE) +
  geom_point(size=2,show.legend=FALSE) +
  facet_wrap(~year) +
  labs(title="Estimating Net Approval (approve-disapprove) for Donald Trump",
       subtitle="Weekly average polls",
       x="Week of the year",
       y="Average Net Approval (%)") +
  geom_hline(yintercept = 0,color="orange") +
  geom_ribbon(aes(ymin=CI_low, ymax=CI_high), linetype=1, alpha=0.1,show.legend=FALSE) +
  scale_x_continuous(breaks = c(13,26,39,52))+
  scale_y_continuous(breaks = c(-20,-17.5,-15,-12.5,-10,-7.5,-5,-2.5,0,2.5,5,7.5))

```

## Compare Confidence Intervals

**Compare the confidence intervals for `week 15` (6-12 April 2020) and `week 34` (17-23 August 2020). Can you explain what's going on? One paragraph would be enough.**

```{r, cache=TRUE}

approval_15_34 <- net_approval_rate_mean_CI %>% 
  filter(year == 2020, week %in% c(15, 34)) %>% 
  mutate(week = c("Week of the 6th of April", "Week of the 17th of August"))

approval_15_34

approval_15_34 %>% ggplot(aes(x = week, y = average_weekly_net_approval, fill = week)) +
  geom_col(show.legend = FALSE, aes(x=reorder(week, desc(average_weekly_net_approval)))) + #reordering so that april is on left
  geom_errorbar(width=.2, aes(ymin=CI_low, ymax=CI_high)) +
  labs(title = "From April to August 2020, Overall a Growing Number of American People Disapprove of Trump",
       x = "",
       y = "Average Net Approval (%)"
       )

approval_15_34 %>% ggplot(aes(x = week, y = CI_high - CI_low, fill = week)) +
  geom_col(show.legend = FALSE, aes(x=reorder(week, CI_high - CI_low))) +
  labs(title = "From April to August 2020, there's Growing Uncertainty in How the American People View Trump",
       subtitle = "Comparing the Confidence Intervals around the Average Net Approval Rate for Two Separate Weeks",
       x = "",
       y = "Length of the 95% Confidence Intervals"
       )
  

```

**As we can see from the two graphs above, the net approval rating confidence interval has increased from week 15 to week 34. Which means as Donald J. Trump's tenure went on, the divergence in the approval rating increased. This could go to show the growing divide between the extremes in the USA and the polarisation of its population. The difference could be due to the emergence of Joe Biden, as the Democratic nominee, and the worsening of the COVID situation in the USA. This could have adversely impacted Trump's net approval rating and be an explanation for the graphs above.**

# Gapminder revisited


```{r, get_data2, cache=TRUE}

library(wbstats)


# load gapminder HIV data
hiv <- read_csv(here::here("data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read.csv(here::here("data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")
worldbank_data <- wb_data(country="countries_only", #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016)

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  wbstats::wb_cachelist$countries

```

##Tidying and Joining Datasets

```{r, get_data3, cache=TRUE}
# A data set is tidy if and only if:
# 
# 1. Every variable is in its own column
# 2. Every observation is in its own row
# 3. Every value is in its own cell (which follows from the above)

life_expectancy
worldbank_data
hiv

#tidying data
 clean_life_expectancy <- life_expectancy %>%
                           pivot_longer(cols=302:2,names_to="Year",values_to = "Life_Expectancy") %>%
                           arrange (Year) %>%
                           mutate(Year = gsub("\\X", "", Year)) %>%
                           rename("Country" = country)

 clean_worldbank_data <- inner_join (worldbank_data, countries) %>%
                         select(date, country, region,NY.GDP.PCAP.KD, SE.PRM.NENR,SH.DYN.MORT,SP.DYN.TFRT.IN)%>%
                         rename("Year" = date,
                             "Country" = country,
                              "GDP_per_capita" = NY.GDP.PCAP.KD,
                               "Primary_school_enrollment"= SE.PRM.NENR,
                                "Mortality_rate" = SH.DYN.MORT,
                                "Female_fertility" =SP.DYN.TFRT.IN,
                              "Region" =region)

 clean_hiv <- hiv %>%
             rename("Country" = country) %>%
             pivot_longer(cols=34:2,names_to="Year",values_to = "HIV_Rate") %>%
             arrange (Year)

 #changing year coloumn from dbl to charachter
 clean_worldbank_data$Year <- as.character(clean_worldbank_data$Year)

 clean_hiv
 clean_life_expectancy
 clean_worldbank_data

 #combining dataframes

 combined_data <- full_join(clean_hiv, clean_life_expectancy, by=c("Year" = "Year","Country"="Country"))

 combined_data <- full_join(combined_data, clean_worldbank_data, by=c("Year" = "Year","Country"="Country"))

 combined_data %>%
           arrange(Country,Year)
          

```

**We used full join because we wanted to keep all values of country and year, even if the HIV data wasn't available early on.**


##1. What is the relationship between HIV prevalence and life expectancy? Generate a scatterplot with a smoothing line to report your results. You may find faceting useful

```{r, get_data5, cache=TRUE}

ggplot(combined_data, aes(x=HIV_Rate, y=Life_Expectancy))+
  geom_point() +
  facet_wrap(~Region)+
  geom_smooth(model = lm)+
  scale_x_log10()+
  scale_y_log10()+
  theme_calc() +
  labs(title="Relationship between HIV Prevalence & Life Expectancy",
       x="HIV Prevalence (per 100 population of age group 15-49)",
       y="Life Expectancy")

```

##1. What is the relationship between fertility rate and GDP per capita? Generate a scatterplot with a smoothing line to report your results. You may find facetting by region useful

```{r, get_data6, cache=TRUE}

ggplot(combined_data, aes(x=Female_fertility, y=GDP_per_capita))+
  geom_point()+
  geom_smooth(model = lm) +
  facet_wrap(~Region)+
  theme_calc() +
  scale_y_log10()+
  scale_x_log10()+
  labs(title="Relationship between Female Fertility & GDP Per Capita",
       x="Female Fertility (number of children born per woman)",
       y="Life Expectancy (in constant 2010 US$)")

```

##1. Which regions have the most observations with missing HIV data? Generate a bar chart (`geom_col()`), in descending order.

```{r, get_data7, cache=TRUE}

combined_data3 <- combined_data %>%
                  mutate(test= is.na(HIV_Rate))%>%
                  filter(test == TRUE) %>%
                  filter (Region != "NA")  %>%
                  group_by(Region) %>%
                  summarise(n_hiv_rate = count(test)) %>%
                  arrange(Region,desc(n_hiv_rate))
combined_data3

ggplot(combined_data3,aes(x=reorder(Region,n_hiv_rate, order = TRUE), y=n_hiv_rate)) +
      geom_col(stat= "identity")+
      coord_flip()+
      theme_clean()+
      labs(title="HIV Data Collected",
       x="Regions",
       y="Years where HIV Data was collected")

  
```



##1. How has mortality rate for under 5 changed by region? In each region, find the top 5 countries that have seen the greatest improvement, as well as those 5 countries where mortality rates have had the least improvement or even deterioration.

```{r, get_data8, cache=TRUE}

#Changing Mortality Rate

mrsum <- combined_data [!is.na(combined_data$Region), ] 
mrsum

mrsum2 <- mrsum %>%
   filter (Region != "NA") %>%
   group_by(Country,Region) 

mrsum2
        
ggplot(mrsum2, aes(x=Year, y=Mortality_rate, color = Country))+
  geom_point(size = 0.1, show.legend=FALSE) +
  geom_smooth(model = lm,show.legend=FALSE)+
  facet_wrap(~Region) +
  labs(title="Mortality Rate by Region",
       x="Years",
       y="Mortality Rate (for under 5 per 1000 live births)")

#Ranking by Mortality Rate

ranked_mortality <- combined_data [!is.na(combined_data$Mortality_rate), ] 

eap <- ranked_mortality %>%
                    group_by(Country)   %>%
  filter(Region == "East Asia & Pacific") %>%
                    arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")

head(eap)
tail(eap)

#Sub Saharan Africa

ssa <- ranked_mortality %>%
                    group_by(Country)   %>%
  filter(Region == "Sub-Saharan Africa") %>%
                    arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")
head(ssa)
tail(ssa)


#North America
na <- ranked_mortality %>%
                    group_by(Country)   %>%
  filter(Region == "North America") %>%
                    arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")
head(na)
tail(na)


#Middle East & North Africa
mena <- ranked_mortality %>%
                        group_by(Country)   %>%
                        filter(Region == "Middle East & North Africa") %>%
                        arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")
head(mena)
tail(mena) 
        

#Europe & Central Asia

eca <- ranked_mortality %>%
                    group_by(Country)   %>%
                        filter(Region == "Europe & Central Asia") %>%
                        arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")

head(eca)
tail(eca) 

#Latin America & Caribbean
lac <- ranked_mortality %>%
                    group_by(Country)   %>%
                        filter(Region == "Latin America & Caribbean") %>%
                        arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")

head(lac)
tail(lac)


#South Asia

sa <- ranked_mortality %>%
                  group_by(Country)   %>%
                        filter(Region == "South Asia") %>%
                        arrange(Country,Year)  %>%
                    filter(Mortality_rate != "NA") %>%
                    filter(row_number()==1 | row_number()==n()) %>%
                    mutate(Percentage_Change_Mortality = (Mortality_rate- lag(Mortality_rate))/lag(Mortality_rate)) %>%
                    arrange(Region,Percentage_Change_Mortality) %>%
                    select(Region,Percentage_Change_Mortality, Country, Year) %>%
                    filter(Percentage_Change_Mortality != "NA")

head(sa)
tail(sa)

```

#1. Is there a relationship between primary school enrollment and fertility rate?
```{r, get_data9, cache=TRUE}


combined_data2 <- combined_data [!is.na(combined_data$Primary_school_enrollment), ] 
combined_data3 <- combined_data2 [!is.na(combined_data2$Female_fertility), ] 


cor(combined_data3$Primary_school_enrollment,combined_data3$Female_fertility)


```

**They are correlated -0.717, which means that they are negatively correlated. The higher the primary school enrolment, the lower the female fertility.**


# Challenge 1: CDC COVID-19 Public Use Data



```{r, cache=TRUE}
# file contains 11 variables and 3.66m rows and is well over 380Mb. 
# It will take time to download

# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom::vroom(url)%>% # If vroom::vroom(url) doesn't work, use read_csv(url)
  clean_names()

covid_data
```

Sample

```{r covid_challenge, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)
```

Given the data we have, I would like you to produce two graphs that show death % rate:

```{r data_clear}

#Remain useful columns and drop unnecessary columns. Drop samples with unknown age and sex.
covid_data_clear <- covid_data %>%
  select(sex, age_group, icu_yn, death_yn, medcond_yn) %>%
  filter(age_group != "Unknown", age_group!= "Missing") %>%
  filter(sex != "Unknown",sex != "Missing", sex!="Other") 

```

1. by age group, sex, and whether the patient had co-morbidities or not
```{r co-mobidities, out.width="100%"}

#There exist some missing data for death_yn and medcond_yn, which are not our focus. Thus, delete these missing samples.
covid_data_clear2 <- covid_data_clear %>%
  select( -icu_yn) %>%
  filter(medcond_yn != "Unknown", medcond_yn != "Missing") %>%
  filter(death_yn != "Unknown", death_yn != "Missing") %>%
  mutate(death_yn = ifelse(death_yn == "Yes", 1, 0)) %>%
  mutate(medcond_yn = case_when(
    medcond_yn=="Yes" ~ "With co-morbidities",
    medcond_yn=="No" ~ "Without co-morbidities")) 
    #rename variables in medcond_yn

#Plot the graph using ggplot
covid_data_clear2 %>% 
  group_by(sex, medcond_yn, age_group) %>% 
  summarise(count=n(),
            death_rate=sum(death_yn)/count) %>%
  ggplot(aes(x=age_group,y=death_rate)) +
  geom_col(show.legend = FALSE, fill="#a6bddb") + 
  geom_text(aes(label=round(death_rate*100,1), vjust=0.5, hjust=-0.1)) +
  facet_grid(medcond_yn ~ sex) +
  coord_flip() +
  theme_bw() +
  labs(title = "Covid death % by age group, sex and presence of co-morbidities ", x="",y = "", caption = "Source: CDC") +
  NULL

```

1. by age group, sex, and whether the patient was admited to Intensive Care Unit (ICU) or not.
```{r ICU, out.width="100%"}

#There exist some missing data for death_yn and icu_yn, which are not our focus. Thus, delete these missing samples.
covid_data_clear3 <- covid_data_clear %>%
  select( -medcond_yn) %>%
  filter(icu_yn != "Unknown", icu_yn != "Missing") %>%
  filter(death_yn != "Unknown", death_yn != "Missing") %>%
  mutate(death_yn = ifelse(death_yn == "Yes", 1, 0)) %>%
  mutate(icu_yn = case_when(
    icu_yn=="Yes" ~ "Admitted to ICU",
    icu_yn=="No" ~ "No ICU")) 
    #rename variables in icu_yn

#Plot the graph using ggplot
covid_data_clear3 %>% 
  group_by(sex, icu_yn, age_group) %>% 
  summarise(count=n(),
            death_rate=sum(death_yn)/count) %>%
  ggplot(aes(x=age_group,y=death_rate)) +
  geom_col(show.legend = FALSE, fill="#fdbb84") + 
  geom_text(aes(label=round(death_rate*100,1), vjust=0.5, hjust=-0.1)) +
  facet_grid(icu_yn ~ sex) +
  coord_flip() +
  theme_bw() +
  labs(title = "Covid death % by age group, sex and whether patient was admitted to ICU", x="",y = "", caption = "Source: CDC") +
  NULL

```


# Challenge 2: Excess rentals in TfL bike sharing


```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```


**We can easily create a facet grid that plots bikes hired by month and year.**


```{r Bikes hired per month,warnings= FALSE, message=FALSE,fig.width=15}
bike_filtered <- bike %>% filter(year>=2015) %>% mutate(month1 = month(day))

ggplot(bike_filtered, aes(x=bikes_hired))+
  geom_density()+
  facet_grid(rows=vars(year),cols=vars(month))+
  labs(title = "Distribution of bikes hired per month",
       x= 'Bike Hired', y = "") + 
  scale_x_continuous(breaks = c(20000, 40000, 60000), 
                labels = c('20K', '40K', '60K'))+
  scale_y_continuous(breaks = c()) +
  theme(legend.position = "none") +
  theme_bw()+
  NULL
```


**Bikes hired in May and June in 2020 compared with previous years dropped significantly, probabily attributed to lockdown.**

##However, the challenge I want you to work on is to reproduce the following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

##The second one looks at percentage changes from the expected level of weekly rentals. The two grey shaded rectangles correspond to the second (weeks 14-26) and fourth (weeks 40-52) quarters.

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

##For both of these graphs, you have to calculate the expected number of rentals per week or month between 2015-2019 and then, see how each week/month of 2020 compares to the expected rentals. Think of the calculation `excess_rentals = actual_rentals - expected_rentals`. 

##Should you use the mean or the median to calculate your expected rentals? Why?

```{r monthly_change, warnings= FALSE, message=FALSE,fig.width=12}
bike15_19 <- bike %>% 
  filter(year>=2015 & year<=2019) %>% 
  mutate(month1 = month(day))
  
#Calculate the monthly average between 2015-2019 using median 
expected_monthly_rentals <- bike15_19 %>% group_by(month1) %>% summarise(median_exp=median(bikes_hired))
#Calculate the acutual monthly average between 2015-2020 using median 
bike_month <- bike_filtered %>% group_by(year,month1) %>% summarise(median_ac=median(bikes_hired)) 
#left_join the dataframe
bike_month_exc <- left_join(bike_month,expected_monthly_rentals,by='month1')
bike_month_exc <- bike_month_exc %>% mutate (maxline=ifelse(median_exp>median_ac,median_exp,median_ac),
                                                   minline=ifelse(median_exp<median_ac,median_exp,median_ac))

# replicate the plot with two ribbons using different colors
ggplot(bike_month_exc,aes(x=month1))+
  geom_ribbon(aes(ymin=minline,ymax=median_exp),fill='lightblue')+
  geom_ribbon(aes(ymin=median_exp,ymax=maxline),fill='lightgreen')+
  geom_line(aes(y=median_exp),color='black',size=1)+
  facet_wrap(~year)+
  labs(title = "Monthly changes in Tfl Bike Rentals",
       x= '', y = "Bike Rentals",caption = "Source: Tfl, London Data Store") +
  scale_x_continuous(breaks = c(1, 2, 3,4,5,6,7,8,9,10,11,12), 
                labels = c('JAN', 'FEB', 'MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC'))+
  theme(axis.text.x = element_text(size = 8, margin = margin(0,15,0,0)))+
  NULL

```

```{r weekly_change, warnings= FALSE, message=FALSE,fig.width=12}
#Calculate the weekly average between 2015-2019 using median 
expected_weekly_rentals <- bike15_19 %>% 
  group_by(week) %>% 
  summarise(median_exp=median(bikes_hired))
#Calculate the actual weekly average between 2015-2020 using median 
bike15_20 <- bike %>% 
  filter(year>=2015 & year<=2020) %>% 
  mutate(month1 = month(day))

bike_week <- bike15_20 %>% 
  group_by(year,week) %>% 
  summarise(median_ac=median(bikes_hired)) 
#left_join the dataframe
bike_week_exc <- left_join(bike_week,expected_weekly_rentals,by='week')
bike_week_exc <- bike_week_exc %>% mutate(bike_week_excess_rate = median_ac/median_exp-1)
bike_week_exc <- bike_week_exc %>% mutate (maxline=ifelse(bike_week_excess_rate>0,bike_week_excess_rate,0),
                                                   minline=ifelse(bike_week_excess_rate<0,bike_week_excess_rate,0),
                                                 whether_pos=ifelse(bike_week_excess_rate>0,'1','0'))
# replicate the plot with two ribbons and a rug using different colors
ggplot(bike_week_exc,aes(x=week))+
  geom_ribbon(aes(ymin=minline,ymax=0),fill='lightblue')+
  geom_ribbon(aes(ymin=0,ymax=maxline),fill='lightgreen')+
  geom_line(aes(y=bike_week_excess_rate),size=0.5) +
  geom_rug(aes(color=whether_pos),sides="b",show.legend = FALSE)+
  scale_color_manual(values = c("lightblue", "lightgreen"))+
  facet_wrap(~year)+
  labs(title = "Weekly changes in Tfl Bike Rentals",
       x= '', y = "week",caption = "Source: Tfl, London Data Store") +
  scale_x_continuous(breaks = c(13,26,39,53), 
                labels = c('13', '26', '39','53'))+
  scale_y_continuous(breaks = c(-0.6,-0.3,0,0.3,0.6), 
                labels = c('-60%', '-30%', '0%','30%','60%'))+
  theme(axis.text.x = element_text(size = 12, margin = margin(0,15,0,0)))+
  NULL
```

##For both of these graphs, you have to calculate the expected number of rentals per week or month between 2015-2019 and then, see how each week/month of 2020 compares to the expected rentals. Think of the calculation `excess_rentals = actual_rentals - expected_rentals`. 

##Should you use the mean or the median to calculate your expected rentals? Why?



# Deliverables



# Details

- Who did you collaborate with: entire group
- Approximately how much time did you spend on this problem set: week
- What, if anything, gave you the most trouble: everything



> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? **yes**


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.